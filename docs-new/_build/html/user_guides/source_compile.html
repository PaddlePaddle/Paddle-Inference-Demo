

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>源码编译 &mdash; Paddle-Inference  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="飞腾/鲲鹏下从源码编译" href="compile_ARM.html" />
    <link rel="prev" title="预测示例 (GO)" href="../quick_start/go_demo.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Paddle-Inference
          

          
          </a>

          
            
            
              <div class="version">
                latest
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p><span class="caption-text">产品介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../product_introduction/summary.html">飞桨推理产品简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../product_introduction/inference_intro.html">Paddle Inference 简介</a></li>
</ul>
<p><span class="caption-text">快速开始</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/workflow.html">预测流程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/cpp_demo.html">预测示例 (C++)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/python_demo.html">预测示例 (Python)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/c_demo.html">预测示例 (C)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/go_demo.html">预测示例 (GO)</a></li>
</ul>
<p><span class="caption-text">使用方法</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">源码编译</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#section-2">什么时候需要源码编译？</a></li>
<li class="toctree-l2"><a class="reference internal" href="#section-3">编译原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="#section-4">编译步骤</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ubuntu-18-04">基于 Ubuntu 18.04</a></li>
<li class="toctree-l2"><a class="reference internal" href="#windows-10">基于 Windows 10</a></li>
<li class="toctree-l2"><a class="reference internal" href="#macosx-10-14">基于 MacOSX 10.14</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="compile_ARM.html"><strong>飞腾/鲲鹏下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="compile_SW.html"><strong>申威下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="compile_ZHAOXIN.html"><strong>兆芯下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="compile_MIPS.html"><strong>龙芯下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="download_lib.html">下载安装Linux预测库</a></li>
<li class="toctree-l1"><a class="reference internal" href="download_lib.html#windows">下载安装Windows预测库</a></li>
<li class="toctree-l1"><a class="reference internal" href="download_lib.html#mac">下载安装Mac预测库</a></li>
</ul>
<p><span class="caption-text">性能调优</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimize/paddle_x86_cpu_int8.html">X86 CPU 上部署量化模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize/paddle_x86_cpu_bf16.html">X86 CPU 上部署BF16预测</a></li>
</ul>
<p><span class="caption-text">工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tools/visual.html">模型可视化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/x2paddle.html">模型转换工具 X2Paddle</a></li>
</ul>
<p><span class="caption-text">硬件部署示例</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/x86_linux_demo.html">X86 Linux上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/x86_windows_demo.html">X86 Windows上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/paddle_xpu_infer_cn.html">使用昆仑预测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/cuda_linux_demo.html">Linux上GPU预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/cuda_jetson_demo.html">NV Jetson上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/cuda_windows_demo.html">Windows上GPU预测部署示例</a></li>
</ul>
<p><span class="caption-text">Benchmark</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/benchmark.html">性能数据</a></li>
</ul>
<p><span class="caption-text">API 文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/cxx_api_index.html">C++ API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/python_api_index.html">Python API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/c_api_index.html">C API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/go_api_index.html">GO API 文档</a></li>
</ul>
<p><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/faq.html">Paddle Inference FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/training_to_deployment.html">训练推理示例说明</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Paddle-Inference</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>源码编译</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/user_guides/source_compile.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="section-1">
<h1>源码编译<a class="headerlink" href="#section-1" title="Permalink to this headline">¶</a></h1>
<section id="section-2">
<h2>什么时候需要源码编译？<a class="headerlink" href="#section-2" title="Permalink to this headline">¶</a></h2>
<p>深度学习的发展十分迅速，对科研或工程人员来说，可能会遇到一些需要自己开发op的场景，可以在python层面编写op，但如果对性能有严格要求的话则必须在C++层面开发op，对于这种情况，需要用户源码编译飞桨，使之生效。
此外对于绝大多数使用C++将模型部署上线的工程人员来说，您可以直接通过飞桨官网下载已编译好的预测库，快捷开启飞桨使用之旅。<a class="reference external" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/advanced_guide/inference_deployment/inference/build_and_install_lib_cn.html">飞桨官网</a> 提供了多个不同环境下编译好的预测库。如果用户环境与官网提供环境不一致（如cuda 、cudnn、tensorrt版本不一致等），或对飞桨源代码有修改需求，或希望进行定制化构建，可查阅本文档自行源码编译得到预测库。</p>
</section>
<section id="section-3">
<h2>编译原理<a class="headerlink" href="#section-3" title="Permalink to this headline">¶</a></h2>
<p><strong>一：目标产物</strong></p>
<p>飞桨框架的源码编译包括源代码的编译和链接，最终生成的目标产物包括：</p>
<blockquote>
<div><ul class="simple">
<li><p>含有 C++ 接口的头文件及其二进制库：用于C++环境，将文件放到指定路径即可开启飞桨使用之旅。</p></li>
<li><p>Python Wheel 形式的安装包：用于Python环境，也就是说，前面讲的pip安装属于在线安装，这里属于本地安装。</p></li>
</ul>
</div></blockquote>
<p><strong>二：基础概念</strong></p>
<p>飞桨主要由C++语言编写，通过pybind工具提供了Python端的接口，飞桨的源码编译主要包括编译和链接两步。
* 编译过程由编译器完成，编译器以编译单元（后缀名为 .cc 或 .cpp 的文本文件）为单位，将 C++ 语言 ASCII 源代码翻译为二进制形式的目标文件。一个工程通常由若干源码文件组织得到，所以编译完成后，将生成一组目标文件。
* 链接过程使分离编译成为可能，由链接器完成。链接器按一定规则将分离的目标文件组合成一个能映射到内存的二进制程序文件，并解析引用。由于这个二进制文件通常包含源码中指定可被外部用户复用的函数接口，所以也被称作函数库。根据链接规则不同，链接可分为静态和动态链接。静态链接对目标文件进行归档；动态链接使用地址无关技术，将链接放到程序加载时进行。
配合包含声明体的头文件（后缀名为 .h 或 .hpp），用户可以复用程序库中的代码开发应用。静态链接构建的应用程序可独立运行，而动态链接程序在加载运行时需到指定路径下搜寻其依赖的二进制库。</p>
<p><strong>三：编译方式</strong></p>
<p>飞桨框架的设计原则之一是满足不同平台的可用性。然而，不同操作系统惯用的编译和链接器是不一样的，使用它们的命令也不一致。比如，Linux 一般使用 GNU 编译器套件（GCC），Windows 则使用 Microsoft Visual C++（MSVC）。为了统一编译脚本，飞桨使用了支持跨平台构建的 CMake，它可以输出上述编译器所需的各种 Makefile 或者 Project 文件。
为方便编译，框架对常用的CMake命令进行了封装，如仿照 Bazel工具封装了 cc_binary 和 cc_library ，分别用于可执行文件和库文件的产出等，对CMake感兴趣的同学可在 cmake/generic.cmake 中查看具体的实现逻辑。Paddle的CMake中集成了生成python wheel包的逻辑，对如何生成wheel包感兴趣的同学可参考 <a class="reference external" href="https://packaging.python.org/tutorials/packaging-projects/">相关文档</a> 。</p>
</section>
<section id="section-4">
<h2>编译步骤<a class="headerlink" href="#section-4" title="Permalink to this headline">¶</a></h2>
<p>飞桨分为 CPU 版本和 GPU 版本。如果您的计算机没有 Nvidia GPU，请选择 CPU 版本构建安装。如果您的计算机含有 Nvidia GPU 且预装有 CUDA / CuDNN，也可选择 GPU 版本构建安装。</p>
<p><strong>推荐配置及依赖项</strong></p>
<p>1、稳定的 Github 连接，主频 1 GHz 以上的多核处理器，9 GB 以上磁盘空间。
2、GCC 版本 4.8 或者 8.2；或者 Visual Studio 2015 Update 3。
3、Python 版本 2.7 或 3.5 以上，pip 版本 9.0 及以上；CMake v3.10 及以上；Git 版本 2.17 及以上。请将可执行文件放入系统环境变量中以方便运行。
4、GPU 版本额外需要 Nvidia CUDA 9 / 10，CuDNN v7 及以上版本。根据需要还可能依赖 TensorRT。</p>
</section>
<section id="ubuntu-18-04">
<h2>基于 Ubuntu 18.04<a class="headerlink" href="#ubuntu-18-04" title="Permalink to this headline">¶</a></h2>
<p><strong>一：环境准备</strong></p>
<p>除了本节开头提到的依赖，在 Ubuntu 上进行飞桨的源码编译，您还需要准备 GCC8 编译器等工具，可使用下列命令安装：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo apt-get install gcc g++ make cmake git vim unrar python3 python3-dev python3-pip swig wget patchelf libopencv-dev
pip3 install numpy protobuf wheel setuptools
</pre></div>
</div>
<p>若需启用 cuda 加速，需准备 cuda、cudnn。上述工具的安装请参考 nvidia 官网，以 cuda10.1，cudnn7.6 为例配置 cuda 环境。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># cuda</span>
sh cuda_10.1.168_418.67_linux.run
<span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span>/usr/local/cuda-10.1/bin<span class="si">${</span><span class="nv">PATH</span><span class="p">:+:</span><span class="si">${</span><span class="nv">PATH</span><span class="si">}}</span>
<span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda-10.1/<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="p">:+:</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}}</span>

<span class="c1"># cudnn</span>
tar -xzvf cudnn-10.1-linux-x64-v7.6.4.38.tgz
sudo cp -a cuda/include/cudnn.h /usr/local/cuda/include/
sudo cp -a cuda/lib64/libcudnn* /usr/local/cuda/lib64/
</pre></div>
</div>
<p><strong>编译飞桨过程中可能会打开很多文件，Ubuntu 18.04 默认设置最多同时打开的文件数是1024（参见 ulimit -a），需要更改这个设定值。</strong></p>
<p>在 /etc/security/limits.conf 文件中添加两行。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>* hard noopen <span class="m">102400</span>
* soft noopen <span class="m">102400</span>
</pre></div>
</div>
<p>重启计算机，重启后执行以下指令，请将${user}切换成当前用户名。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>su <span class="si">${</span><span class="nv">user</span><span class="si">}</span>
<span class="nb">ulimit</span> -n <span class="m">102400</span>
</pre></div>
</div>
<p>若在 TensorRT 依赖编译过程中出现头文件虚析构函数报错，请在 NvInfer.h 文件中为 class IPluginFactory 和 class IGpuAllocator 分别添加虚析构函数：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">virtual</span> <span class="o">~</span><span class="n">IPluginFactory</span><span class="p">()</span> <span class="p">{};</span>
<span class="k">virtual</span> <span class="o">~</span><span class="n">IGpuAllocator</span><span class="p">()</span> <span class="p">{};</span>
</pre></div>
</div>
<p><strong>二：编译命令</strong></p>
<p>使用 Git 将飞桨代码克隆到本地，并进入目录，切换到稳定版本（git tag显示的标签名，如 release/2.0）。
<strong>飞桨使用 develop 分支进行最新特性的开发，使用 release 分支发布稳定版本。在 GitHub 的 Releases 选项卡中，可以看到飞桨版本的发布记录。</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/PaddlePaddle/Paddle.git
<span class="nb">cd</span> Paddle
git checkout release/2.0
</pre></div>
</div>
<p>下面以 GPU 版本为例说明编译命令。其他环境可以参考“CMake编译选项表”修改对应的cmake选项。比如，若编译 CPU 版本，请将 WITH_GPU 设置为 OFF。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 创建并进入 build 目录</span>
mkdir build_cuda <span class="o">&amp;&amp;</span> <span class="nb">cd</span> build_cuda
<span class="c1"># 执行cmake指令</span>
cmake .. -DPY_VERSION<span class="o">=</span><span class="m">3</span> <span class="se">\</span>
        -DWITH_TESTING<span class="o">=</span>OFF <span class="se">\</span>
        -DWITH_MKL<span class="o">=</span>ON <span class="se">\</span>
        -DWITH_GPU<span class="o">=</span>ON <span class="se">\</span>
        -DON_INFER<span class="o">=</span>ON <span class="se">\</span>
        ..
</pre></div>
</div>
<p><strong>使用make编译</strong></p>
<p>make -j4</p>
<p><strong>编译成功后可在dist目录找到生成的.whl包</strong></p>
<p>pip3 install python/dist/paddlepaddle-2.0.0-cp38-cp38-linux_x86_64.whl</p>
<p><strong>预测库编译</strong></p>
<p>make inference_lib_dist -j4</p>
<p><strong>cmake编译环境表</strong></p>
<p>以下介绍的编译方法都是通用步骤，根据环境对应修改cmake选项即可。</p>
<p><strong>三：NVIDIA Jetson嵌入式硬件预测库源码编译</strong></p>
<p>NVIDIA Jetson是NVIDIA推出的嵌入式AI平台，Paddle Inference支持在 NVIDIA Jetson平台上编译预测库。具体步骤如下：</p>
<p>1、准备环境：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 开启硬件性能模式</span>
sudo nvpmodel -m <span class="m">0</span> <span class="o">&amp;&amp;</span> sudo jetson_clocks
<span class="c1"># 增加 DDR 可用空间，Xavier 默认内存为 16 GB，所以内存足够，如在 Nano 上尝试，请执行如下操作。</span>
sudo fallocate -l 5G /var/swapfile
sudo chmod <span class="m">600</span> /var/swapfile
sudo mkswap /var/swapfile
sudo swapon /var/swapfile
sudo bash -c <span class="s1">&#39;echo &quot;/var/swapfile swap swap defaults 0 0&quot; &gt;&gt; /etc/fstab&#39;</span>
</pre></div>
</div>
<p>2、编译预测库：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> Paddle
mkdir build
<span class="nb">cd</span> build
cmake .. <span class="se">\</span>
-DWITH_CONTRIB<span class="o">=</span>OFF <span class="se">\</span>
-DWITH_MKL<span class="o">=</span>OFF  <span class="se">\</span>
-DWITH_MKLDNN<span class="o">=</span>OFF <span class="se">\</span>
-DWITH_TESTING<span class="o">=</span>OFF <span class="se">\</span>
-DCMAKE_BUILD_TYPE<span class="o">=</span>Release <span class="se">\</span>
-DON_INFER<span class="o">=</span>ON <span class="se">\</span>
-DWITH_PYTHON<span class="o">=</span>OFF <span class="se">\</span>
-DWITH_XBYAK<span class="o">=</span>OFF  <span class="se">\</span>
-DWITH_NV_JETSON<span class="o">=</span>ON
make -j4

<span class="c1"># 生成预测lib</span>
make inference_lib_dist -j4
</pre></div>
</div>
<p>3、参照 <a class="reference external" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/advanced_guide/performance_improving/inference_improving/paddle_tensorrt_infer.html#id2">官网样例</a> 进行测试。</p>
</section>
<section id="windows-10">
<h2>基于 Windows 10<a class="headerlink" href="#windows-10" title="Permalink to this headline">¶</a></h2>
<p><strong>一：环境准备</strong></p>
<p>除了本节开头提到的依赖，在 Windows 10 上编译飞桨，您还需要准备 Visual Studio 2015 Update 3。飞桨正在对更高版本的编译支持做完善支持。</p>
<p>在命令提示符输入下列命令，安装必需的 Python 组件。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip3 install numpy protobuf wheel
</pre></div>
</div>
<p><strong>二：编译命令</strong></p>
<p>使用 Git 将飞桨代码克隆到本地，并进入目录，切换到稳定版本（git tag显示的标签名，如 release/2.0）。
<strong>飞桨使用 develop 分支进行最新特性的开发，使用 release 分支发布稳定版本。在 GitHub 的 Releases 选项卡中，可以看到飞桨版本的发布记录。</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/PaddlePaddle/Paddle.git
<span class="nb">cd</span> Paddle
git checkout release/2.0
</pre></div>
</div>
<p>创建一个构建目录，并在其中执行 CMake，生成解决方案文件 Solution File，以编译 CPU 版本为例说明编译命令，其他环境可以参考“CMake编译选项表”修改对应的cmake选项。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir build
<span class="nb">cd</span> build
cmake .. -G <span class="s2">&quot;Visual Studio 14 2015 Win64&quot;</span> -A x64 -DWITH_GPU<span class="o">=</span>OFF -DWITH_TESTING<span class="o">=</span>OFF -DON_INFER<span class="o">=</span>ON
        -DCMAKE_BUILD_TYPE<span class="o">=</span>Release -DPY_VERSION<span class="o">=</span><span class="m">3</span>
</pre></div>
</div>
<p>使用 Visual Studio 打开解决方案文件，在窗口顶端的构建配置菜单中选择 Release x64，单击生成解决方案，等待构建完毕即可。</p>
<p><strong>cmake编译环境表</strong></p>
<p><strong>结果验证</strong></p>
<p><strong>一：python whl包</strong></p>
<p>编译完毕后，会在 python/dist 目录下生成一个 Python Wheel 安装包，安装测试的命令为：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip3 install paddlepaddle-2.0.0-cp38-cp38-win_amd64.whl
</pre></div>
</div>
<p>安装完成后，可以使用 python3 进入python解释器，输入以下指令，出现 <a href="#system-message-1"><span class="problematic" id="problematic-1">`</span></a>Your Paddle Fluid is installed successfully! ` ，说明安装成功。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">paddle.fluid</span> <span class="k">as</span> <span class="nn">fluid</span>
<span class="n">fluid</span><span class="o">.</span><span class="n">install_check</span><span class="o">.</span><span class="n">run_check</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>二：c++ lib</strong></p>
<p>预测库编译后，所有产出均位于build目录下的paddle_inference_install_dir目录内，目录结构如下。version.txt 中记录了该预测库的版本信息，包括Git Commit ID、使用OpenBlas或MKL数学库、CUDA/CUDNN版本号。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>build/paddle_inference_install_dir
├── CMakeCache.txt
├── paddle
│   ├── include
│   │   ├── paddle_anakin_config.h
│   │   ├── paddle_analysis_config.h
│   │   ├── paddle_api.h
│   │   ├── paddle_inference_api.h
│   │   ├── paddle_mkldnn_quantizer_config.h
│   │   └── paddle_pass_builder.h
│   └── lib
│       ├── libpaddle_inference.a <span class="o">(</span>Linux<span class="o">)</span>
│       ├── libpaddle_inference.so <span class="o">(</span>Linux<span class="o">)</span>
│       └── libpaddle_inference.lib <span class="o">(</span>Windows<span class="o">)</span>
├── third_party
│   ├── boost
│   │   └── boost
│   ├── eigen3
│   │   ├── Eigen
│   │   └── unsupported
│   └── install
│       ├── gflags
│       ├── glog
│       ├── mkldnn
│       ├── mklml
│       ├── protobuf
│       ├── xxhash
│       └── zlib
└── version.txt
</pre></div>
</div>
<p>Include目录下包括了使用飞桨预测库需要的头文件，lib目录下包括了生成的静态库和动态库，third_party目录下包括了预测库依赖的其它库文件。</p>
<p>您可以编写应用代码，与预测库联合编译并测试结果。请参考 <a class="reference external" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/guides/05_inference_deployment/inference/native_infer.html">C++ 预测库 API 使用</a> 一节。</p>
</section>
<section id="macosx-10-14">
<h2>基于 MacOSX 10.14<a class="headerlink" href="#macosx-10-14" title="Permalink to this headline">¶</a></h2>
<p><strong>一：环境准备</strong></p>
<p>在编译 Paddle 前，需要在 MacOSX 预装 Apple Clang 11.0 和 Python 3.8，以及 python-pip。请使用下列命令安装 Paddle 编译必需的 Python 组件包。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip3 install numpy protobuf wheel setuptools
</pre></div>
</div>
<p><strong>二：编译命令</strong></p>
<p>使用 Git 将飞桨代码克隆到本地，并进入目录，切换到稳定版本（git tag显示的标签名，如 release/2.0）。
<strong>飞桨使用 develop 分支进行最新特性的开发，使用 release 分支发布稳定版本。在 GitHub 的 Releases 选项卡中，可以看到飞桨版本的发布记录。</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/PaddlePaddle/Paddle.git
<span class="nb">cd</span> Paddle
git checkout release/2.0
</pre></div>
</div>
<p>下面以 CPU-MKL 版本为例说明编译命令。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 创建并进入 build 目录</span>
mkdir build <span class="o">&amp;&amp;</span> <span class="nb">cd</span> build
<span class="c1"># 执行cmake指令</span>
cmake .. -DPY_VERSION<span class="o">=</span><span class="m">3</span> <span class="se">\</span>
        -DWITH_TESTING<span class="o">=</span>OFF <span class="se">\</span>
        -DWITH_MKL<span class="o">=</span>ON <span class="se">\</span>
        -DON_INFER<span class="o">=</span>ON <span class="se">\</span>
        ..
</pre></div>
</div>
<p><strong>使用make编译</strong></p>
<p>make -j4</p>
<p><strong>编译成功后可在dist目录找到生成的.whl包</strong></p>
<p>pip3 install python/dist/paddlepaddle-2.0.0-cp38-cp38-macosx_10_14_x86_64.whl</p>
<p><strong>预测库编译</strong></p>
<p>make inference_lib_dist -j4</p>
<p><strong>cmake编译环境表</strong></p>
<p>以下介绍的编译方法都是通用步骤，根据环境对应修改cmake选项即可。</p>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="compile_ARM.html" class="btn btn-neutral float-right" title="飞腾/鲲鹏下从源码编译" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../quick_start/go_demo.html" class="btn btn-neutral float-left" title="预测示例 (GO)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Paddle-Inference Developer

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>