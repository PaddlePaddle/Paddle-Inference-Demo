

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>使用Paddle-TensorRT库预测 &mdash; Paddle-Inference  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Paddle-Inference
          

          
          </a>

          
            
            
              <div class="version">
                latest
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p><span class="caption-text">产品介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../product_introduction/summary.html">飞桨推理产品简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../product_introduction/inference_intro.html">Paddle Inference 简介</a></li>
</ul>
<p><span class="caption-text">快速开始</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/workflow.html">预测流程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/cpp_demo.html">预测示例 (C++)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/python_demo.html">预测示例 (Python)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/c_demo.html">预测示例 (C)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/go_demo.html">预测示例 (GO)</a></li>
</ul>
<p><span class="caption-text">使用方法</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/source_compile.html">源码编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/compile_ARM.html"><strong>飞腾/鲲鹏下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/compile_SW.html"><strong>申威下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/compile_ZHAOXIN.html"><strong>兆芯下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/compile_MIPS.html"><strong>龙芯下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/download_lib.html">下载安装Linux预测库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/download_lib.html#windows">下载安装Windows预测库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/download_lib.html#mac">下载安装Mac预测库</a></li>
</ul>
<p><span class="caption-text">性能调优</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="paddle_x86_cpu_int8.html">X86 CPU 上部署量化模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="paddle_x86_cpu_bf16.html">X86 CPU 上部署BF16预测</a></li>
</ul>
<p><span class="caption-text">工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tools/visual.html">模型可视化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/x2paddle.html">模型转换工具 X2Paddle</a></li>
</ul>
<p><span class="caption-text">硬件部署示例</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/x86_linux_demo.html">X86 Linux上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/x86_windows_demo.html">X86 Windows上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/paddle_xpu_infer_cn.html">使用昆仑预测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/cuda_linux_demo.html">Linux上GPU预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/cuda_jetson_demo.html">NV Jetson上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/cuda_windows_demo.html">Windows上GPU预测部署示例</a></li>
</ul>
<p><span class="caption-text">Benchmark</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/benchmark.html">性能数据</a></li>
</ul>
<p><span class="caption-text">API 文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/cxx_api_index.html">C++ API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/python_api_index.html">Python API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/c_api_index.html">C API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/go_api_index.html">GO API 文档</a></li>
</ul>
<p><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/faq.html">Paddle Inference FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/training_to_deployment.html">训练推理示例说明</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Paddle-Inference</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>使用Paddle-TensorRT库预测</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/optimize/paddle_trt_ch.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="paddle-tensorrt">
<h1>使用Paddle-TensorRT库预测<a class="headerlink" href="#paddle-tensorrt" title="Permalink to this headline">¶</a></h1>
<p>NVIDIA TensorRT 是一个高性能机器学习推理SDK，专注于深度学习模型在NVIDIA硬件的快速高效的推理。PaddlePaddle 以子图方式集成了TensorRT，将可用TensorRT加速的算子组成子图供给TensorRT，以获取TensorRT加速的同时，保留paddlepaddle即训即推的能力。在这篇文章中，我们会介绍如何使用Paddle-TRT加速预测。</p>
<p>如果您需要安装 <a class="reference external" href="https://developer.nvidia.com/nvidia-tensorrt-6x-download">TensorRT</a>，请参考 <a class="reference external" href="https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-601/tensorrt-install-guide/index.html">trt文档</a>.</p>
<section id="section-1">
<h2>概述<a class="headerlink" href="#section-1" title="Permalink to this headline">¶</a></h2>
<p>当模型加载后，神经网络可以表示为由变量和运算节点组成的计算图。当打开TRT子图模式时，，Paddle会在图分析阶段检测模型中可以使用TensorRT优化的子图并将其替换为TensorRT节点。在模型的推断期间，如果遇到TensorRT节点，Paddle会调用TensorRT库对该节点进行推理，其他的节点调用Paddle的原生实现。</p>
<p>目前Paddle-TRT支持静态shape、动态shape两种运行方式。静态shape主要用于模型输入size除batch维，其他维度信息不变的情况；动态shape可用于输入size任意变化的模型， 比如NLP、OCR等领域模型的支持，当然也包括静态shape支持的模型； 静态shape 和动态shape 都支持fp32、fp16、int8等多种计算精度；Paddle-TRT 支持服务器端GPU，如T4、A10， 也支持边缘端硬件，如Jetson NX、 Jetson Nano、 Jetson TX2等。 在边缘硬件上，除支持常规的GPU外，还可以使用DLA进行推理；也支持RTX2080，3090等游戏显卡；</p>
<p>因使用TensorRT首次推理时，TensorRT需要进行各OP融合、显存复用、以及OP的kernel选择等，导致首帧耗时过长，Paddle-TRT开放了序列化接口，用于将TensorRT分析的信息进行存储，在后续推理直接载入相关序列化信息，从而减少启动耗时；</p>
<p><strong>Note:</strong></p>
<ol class="arabic simple">
<li><p>从源码编译时，TensorRT预测库目前仅支持使用GPU编译，且需要设置编译选项TENSORRT_ROOT为TensorRT所在的路径。</p></li>
<li><p>Windows支持需要TensorRT 版本5.0以上。</p></li>
<li><p>使用Paddle-TRT的动态shape输入功能要求TRT的版本在6.0以上。</p></li>
</ol>
</section>
<section id="section-2">
<h2>一：环境准备<a class="headerlink" href="#section-2" title="Permalink to this headline">¶</a></h2>
<p>使用Paddle-TRT功能，我们需要准备带TensorRT的Paddle运行环境，我们提供了以下几种方式：</p>
<p>1）linux下通过pip安装</p>
<p>请从 <a class="reference external" href="https://paddle-inference.readthedocs.io/en/latest/user_guides/download_lib.html">whl list</a> 下载带TensorRT且与自己环境一致的whl包，并通过pip安装</p>
<p>2）使用docker镜像</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 拉取镜像，该镜像预装Paddle 2.2 Python环境，并包含c++的预编译库，lib存放在主目录～/ 下。</span>
docker pull paddlepaddle/paddle:latest-dev-cuda11.0-cudnn8-gcc82

sudo nvidia-docker run --name your_name -v <span class="nv">$PWD</span>:/paddle  --network<span class="o">=</span>host -it paddlepaddle/paddle:latest-dev-cuda11.0-cudnn8-gcc82  /bin/bash
</pre></div>
</div>
<p>3）手动编译
编译的方式请参照 <a class="reference external" href="../user_guides/source_compile.html">编译文档</a></p>
<p><strong>Note1：</strong> cmake 期间请设置 TENSORRT_ROOT （即TRT lib的路径）， WITH_PYTHON （是否产出python whl包， 设置为ON）选项。</p>
</section>
<section id="api">
<h2>二：API使用介绍<a class="headerlink" href="#api" title="Permalink to this headline">¶</a></h2>
<p>在 <a class="reference external" href="https://paddleinference.paddlepaddle.org.cn/quick_start/workflow.html">预测流程</a> 一节中，我们了解到Paddle Inference预测包含了以下几个方面：</p>
<ul class="simple">
<li><p>配置推理选项</p></li>
<li><p>创建predictor</p></li>
<li><p>准备模型输入</p></li>
<li><p>模型推理</p></li>
<li><p>获取模型输出</p></li>
</ul>
<p>使用Paddle-TRT 也是遵照这样的流程。我们先用一个简单的例子来介绍这一流程（我们假设您已经对Paddle Inference有一定的了解，如果您刚接触Paddle Inference，请访问 <a class="reference external" href="https://paddleinference.paddlepaddle.org.cn/quick_start/workflow.html">这里</a> 对Paddle Inference有个初步认识。）：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">paddle.inference</span> <span class="k">as</span> <span class="nn">paddle_infer</span>

<span class="k">def</span> <span class="nf">create_predictor</span><span class="p">():</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">paddle_infer</span><span class="o">.</span><span class="n">Config</span><span class="p">(</span><span class="s2">&quot;./resnet50/model&quot;</span><span class="p">,</span> <span class="s2">&quot;./resnet50/params&quot;</span><span class="p">)</span>
    <span class="n">config</span><span class="o">.</span><span class="n">enable_memory_optim</span><span class="p">()</span>
    <span class="n">config</span><span class="o">.</span><span class="n">enable_use_gpu</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># 打开TensorRT。此接口的详细介绍请见下文</span>
    <span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span><span class="n">workspace_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span><span class="p">,</span>
                                  <span class="n">max_batch_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                                  <span class="n">min_subgraph_size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                                  <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Float32</span><span class="p">,</span>
                                  <span class="n">use_static</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">predictor</span> <span class="o">=</span> <span class="n">paddle_infer</span><span class="o">.</span><span class="n">create_predictor</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">predictor</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">predictor</span><span class="p">,</span> <span class="n">img</span><span class="p">):</span>
    <span class="c1"># 准备输入</span>
    <span class="n">input_names</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">get_input_names</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span>  <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_names</span><span class="p">):</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">get_input_handle</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">copy_from_cpu</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
    <span class="c1"># 预测</span>
    <span class="n">predictor</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># 获取输出</span>
    <span class="n">output_names</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">get_output_names</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_names</span><span class="p">):</span>
        <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">get_output_handle</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">output_data</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="o">.</span><span class="n">copy_to_cpu</span><span class="p">()</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">create_predictor</span><span class="p">()</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="p">[</span><span class="n">img</span><span class="p">])</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;class index: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
<p>通过例子我们可以看出，我们通过 <cite>enable_tensorrt_engine</cite> 接口来打开TensorRT选项的。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span><span class="n">workspace_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span><span class="p">,</span>
                              <span class="n">max_batch_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                              <span class="n">min_subgraph_size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                              <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Float32</span><span class="p">,</span>
                              <span class="n">use_static</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>接下来让我们看下该接口中各个参数的作用:</p>
<ul class="simple">
<li><p><strong>workspace_size</strong>，类型：int，默认值为1 &lt;&lt; 30 （1G）。指定TensorRT使用的工作空间大小，TensorRT会在该大小限制下筛选最优的kernel进行推理。</p></li>
<li><p><strong>max_batch_size</strong>，类型：int，默认值为1。需要提前设置最大的batch大小，运行时batch大小不得超过此限定值。</p></li>
<li><p><strong>min_subgraph_size</strong>，类型：int，默认值为3。Paddle-TRT是以子图的形式运行，为了避免性能损失，当子图内部节点个数大于 min_subgraph_size 的时候，才会使用Paddle-TRT运行。</p></li>
<li><p><strong>precision_mode</strong>，类型：<strong>paddle_infer.PrecisionType</strong>, 默认值为 <strong>paddle_infer.PrecisionType.Float32</strong>。指定使用TRT的精度，支持FP32（Float32），FP16（Half），Int8（Int8）。若需要使用Paddle-TRT int8离线量化校准，需设定precision为 <strong>paddle_infer.PrecisionType.Int8</strong> , 且设置 <strong>use_calib_mode</strong> 为True。</p></li>
<li><p><strong>use_static</strong>，类型：bool, 默认值为False。如果指定为True，在初次运行程序的时候会将TRT的优化信息进行序列化到磁盘上，下次运行时直接加载优化的序列化信息而不需要重新生成。</p></li>
<li><p><strong>use_calib_mode</strong>，类型：bool, 默认值为False。若要运行Paddle-TRT int8离线量化校准，需要将此选项设置为True。</p></li>
</ul>
<section id="int8">
<h3>Int8量化预测<a class="headerlink" href="#int8" title="Permalink to this headline">¶</a></h3>
<p>深度学习模型的权重参数在一定程度上是冗余的，在很多任务上，我们可以将模型量化而不影响计算精度。模型量化，一方面可以减少访存、提升计算效率，另一方面，可以降低显存占用。使用Int8量化预测的流程可以分为两步：1）产出量化模型；2）加载量化模型进行推理。下面我们对使用Paddle-TRT进行Int8量化推理的完整流程进行详细介绍。</p>
<p><strong>1. 产出量化模型</strong></p>
<p>目前，我们支持通过两种方式产出量化模型：</p>
<ol class="loweralpha simple">
<li><p>使用TensorRT自带Int8离线量化校准功能。校准即基于训练好的FP32模型和少量校准数据（如500～1000张图片）生成校准表（Calibration table）。推理时，加载FP32模型和此校准表即可使用Int8精度推理。生成校准表的方法如下：</p></li>
</ol>
<blockquote>
<div><ul>
<li><p>指定TensorRT配置时，将 <strong>precision_mode</strong> 设置为 <strong>paddle_infer.PrecisionType.Int8</strong> 并且设置 <strong>use_calib_mode</strong> 为 <strong>True</strong>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span>
  <span class="n">workspace_size</span><span class="o">=</span><span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">30</span><span class="p">,</span>
  <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_subgraph_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Int8</span><span class="p">,</span>
  <span class="n">use_static</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>准备500张左右的真实输入数据，在上述配置下，运行模型。（Paddle-TRT会统计模型中每个tensor值的范围信息，并将其记录到校准表中，运行结束后，会将校准表写入模型目录下的 <cite>_opt_cache</cite> 目录中）</p></li>
</ul>
<p>如果想要了解使用TensorRT自带Int8离线量化校准功能生成校准表的完整代码，请参考 <a href="#system-message-1"><span class="problematic" id="problematic-1">`</span></a>&lt;<a class="reference external" href="https://github.com/PaddlePaddle/Paddle-Inference-Demo/blob/master/c%2B%2B/paddle-trt/trt_gen_calib_table_test.cc">https://github.com/PaddlePaddle/Paddle-Inference-Demo/blob/master/c%2B%2B/paddle-trt/trt_gen_calib_table_test.cc</a>&gt;`_的demo。</p>
</div></blockquote>
<ol class="loweralpha simple" start="2">
<li><p>使用模型压缩工具库PaddleSlim产出量化模型。PaddleSlim支持离线量化和在线量化功能，其中，离线量化与TensorRT离线量化校准原理相似；在线量化又称量化训练(Quantization Aware Training, QAT)，是基于较多数据（如&gt;=5000张图片）对预训练模型进行重新训练，使用模拟量化的思想，在训练阶段更新权重，实现减小量化误差的方法。使用PaddleSlim产出量化模型可以参考文档：</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>离线量化 <a class="reference external" href="https://paddlepaddle.github.io/PaddleSlim/quick_start/quant_post_tutorial.html">快速开始教程</a></p></li>
<li><p>离线量化 <a class="reference external" href="https://paddlepaddle.github.io/PaddleSlim/api_cn/quantization_api.html#quant-post">API接口说明</a></p></li>
<li><p>离线量化 <a class="reference external" href="https://github.com/PaddlePaddle/PaddleSlim/tree/release/1.1.0/demo/quant/quant_post">Demo</a></p></li>
<li><p>量化训练 <a class="reference external" href="https://github.com/PaddlePaddle/PaddleSlim/blob/develop/docs/zh_cn/quick_start/dygraph/dygraph_quant_aware_training_tutorial.md">快速开始教程</a></p></li>
<li><p>量化训练 <a class="reference external" href="https://paddlepaddle.github.io/PaddleSlim/api_cn/quantization_api.html#quant-aware">API接口说明</a></p></li>
<li><p>量化训练 <a class="reference external" href="https://github.com/PaddlePaddle/PaddleSlim/tree/release/1.1.0/demo/quant/quant_aware">Demo</a></p></li>
</ul>
</div></blockquote>
<p>离线量化的优点是无需重新训练，简单易用，但量化后精度可能受影响；量化训练的优点是模型精度受量化影响较小，但需要重新训练模型，使用门槛稍高。在实际使用中，我们推荐先使用TRT离线量化校准功能生成量化模型，若精度不能满足需求，再使用PaddleSlim产出量化模型。</p>
<p><strong>2. 加载量化模型进行Int8预测</strong></p>
<blockquote>
<div><p>加载量化模型进行Int8预测，需要在指定TensorRT配置时，将 <strong>precision_mode</strong> 设置为 <strong>paddle_infer.PrecisionType.Int8</strong> 。</p>
<p>若使用的量化模型为TRT离线量化校准产出的，需要将 <strong>use_calib_mode</strong> 设为 <strong>True</strong> ：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span>
  <span class="n">workspace_size</span><span class="o">=</span><span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">30</span><span class="p">,</span>
  <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_subgraph_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Int8</span><span class="p">,</span>
  <span class="n">use_static</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>完整demo请参考 <a class="reference external" href="https://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/c%2B%2B/paddle-trt/README.md#%E5%8A%A0%E8%BD%BD%E6%A0%A1%E5%87%86%E8%A1%A8%E6%89%A7%E8%A1%8Cint8%E9%A2%84%E6%B5%8B">这里</a> 。</p>
<p>若使用的量化模型为PaddleSlim量化产出的，需要将 <strong>use_calib_mode</strong> 设为 <strong>False</strong> ：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span>
  <span class="n">workspace_size</span><span class="o">=</span><span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">30</span><span class="p">,</span>
  <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_subgraph_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Int8</span><span class="p">,</span>
  <span class="n">use_static</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>完整demo请参考 <a class="reference external" href="https://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/c%2B%2B/paddle-trt/README.md#%E4%B8%89%E4%BD%BF%E7%94%A8trt-%E5%8A%A0%E8%BD%BDpaddleslim-int8%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B">这里</a> 。</p>
</div></blockquote>
</section>
<section id="dynamic-shape">
<h3>运行Dynamic shape<a class="headerlink" href="#dynamic-shape" title="Permalink to this headline">¶</a></h3>
<p>从1.8 版本开始， Paddle对TRT子图进行了Dynamic shape的支持。
使用接口如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span>
        <span class="n">workspace_size</span> <span class="o">=</span> <span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">30</span><span class="p">,</span>
        <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_subgraph_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Float32</span><span class="p">,</span>
        <span class="n">use_static</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">min_input_shape</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]}</span>
<span class="n">max_input_shape</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">]}</span>
<span class="n">opt_input_shape</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">]}</span>

<span class="n">config</span><span class="o">.</span><span class="n">set_trt_dynamic_shape_info</span><span class="p">(</span><span class="n">min_input_shape</span><span class="p">,</span> <span class="n">max_input_shape</span><span class="p">,</span> <span class="n">opt_input_shape</span><span class="p">)</span>
</pre></div>
</div>
<p>从上述使用方式来看，在 config.enable_tensorrt_engine 接口的基础上，新加了一个config.set_trt_dynamic_shape_info 的接口。</p>
<p>该接口用来设置模型输入的最小，最大，以及最优的输入shape。 其中，最优的shape处于最小最大shape之间，在预测初始化期间，会根据opt shape对op选择最优的kernel。</p>
<p>调用了 <strong>config.set_trt_dynamic_shape_info</strong> 接口，预测器会运行TRT子图的动态输入模式，运行期间可以接受最小，最大shape间的任意的shape的输入数据。</p>
</section>
</section>
<section id="section-3">
<h2>三：测试样例<a class="headerlink" href="#section-3" title="Permalink to this headline">¶</a></h2>
<p>我们在github上提供了使用TRT子图预测的更多样例：</p>
<ul class="simple">
<li><p>Python 样例请访问此处 <a class="reference external" href="https://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/python/paddle_trt">链接</a> 。</p></li>
<li><p>C++ 样例地址请访问此处 <a class="reference external" href="https://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/c%2B%2B/paddle-trt">链接</a> 。</p></li>
</ul>
</section>
<section id="paddle-trt">
<h2>四：Paddle-TRT子图运行原理<a class="headerlink" href="#paddle-trt" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>PaddlePaddle采用子图的形式对TensorRT进行集成，当模型加载后，神经网络可以表示为由变量和运算节点组成的计算图。Paddle TensorRT实现的功能是对整个图进行扫描，发现图中可以使用TensorRT优化的子图，并使用TensorRT节点替换它们。在模型的推断期间，如果遇到TensorRT节点，Paddle会调用TensorRT库对该节点进行优化，其他的节点调用Paddle的原生实现。TensorRT在推断期间能够进行Op的横向和纵向融合，过滤掉冗余的Op，并对特定平台下的特定的Op选择合适的kernel等进行优化，能够加快模型的预测速度。</p>
</div></blockquote>
<p>下图使用一个简单的模型展示了这个过程：</p>
<p><strong>原始网络</strong></p>
<blockquote>
<div><img alt="https://raw.githubusercontent.com/NHZlX/FluidDoc/add_trt_doc/doc/fluid/user_guides/howto/inference/image/model_graph_original.png" src="https://raw.githubusercontent.com/NHZlX/FluidDoc/add_trt_doc/doc/fluid/user_guides/howto/inference/image/model_graph_original.png" />
</div></blockquote>
<p><strong>转换的网络</strong></p>
<blockquote>
<div><blockquote>
<div><img alt="https://raw.githubusercontent.com/NHZlX/FluidDoc/add_trt_doc/doc/fluid/user_guides/howto/inference/image/model_graph_trt.png" src="https://raw.githubusercontent.com/NHZlX/FluidDoc/add_trt_doc/doc/fluid/user_guides/howto/inference/image/model_graph_trt.png" />
</div></blockquote>
<p>我们可以在原始模型网络中看到，绿色节点表示可以被TensorRT支持的节点，红色节点表示网络中的变量，黄色表示Paddle只能被Paddle原生实现执行的节点。那些在原始网络中的绿色节点被提取出来汇集成子图，并由一个TensorRT节点代替，成为转换后网络中的 <strong>block-25</strong> 节点。在网络运行过程中，如果遇到该节点，Paddle将调用TensorRT库来对其执行。</p>
</div></blockquote>
</section>
</section>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Paddle-Inference Developer

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>