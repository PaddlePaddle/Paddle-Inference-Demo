

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Using the Paddle-TensorRT Repository for Inference &mdash; Paddle-Inference  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Paddle-Inference
          

          
          </a>

          
            
            
              <div class="version">
                latest
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p><span class="caption-text">产品介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../product_introduction/summary.html">飞桨推理产品简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../product_introduction/inference_intro.html">Paddle Inference 简介</a></li>
</ul>
<p><span class="caption-text">快速开始</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/workflow.html">预测流程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/cpp_demo.html">预测示例 (C++)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/python_demo.html">预测示例 (Python)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/c_demo.html">预测示例 (C)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/go_demo.html">预测示例 (GO)</a></li>
</ul>
<p><span class="caption-text">使用方法</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/source_compile.html">源码编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/compile_ARM.html"><strong>飞腾/鲲鹏下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/compile_SW.html"><strong>申威下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/compile_ZHAOXIN.html"><strong>兆芯下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/compile_MIPS.html"><strong>龙芯下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/download_lib.html">下载安装Linux预测库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/download_lib.html#windows">下载安装Windows预测库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/download_lib.html#mac">下载安装Mac预测库</a></li>
</ul>
<p><span class="caption-text">性能调优</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="paddle_x86_cpu_int8.html">X86 CPU 上部署量化模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="paddle_x86_cpu_bf16.html">X86 CPU 上部署BF16预测</a></li>
</ul>
<p><span class="caption-text">工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tools/visual.html">模型可视化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/x2paddle.html">模型转换工具 X2Paddle</a></li>
</ul>
<p><span class="caption-text">硬件部署示例</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/x86_linux_demo.html">X86 Linux上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/x86_windows_demo.html">X86 Windows上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/paddle_xpu_infer_cn.html">使用昆仑预测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/cuda_linux_demo.html">Linux上GPU预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/cuda_jetson_demo.html">NV Jetson上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo_tutorial/cuda_windows_demo.html">Windows上GPU预测部署示例</a></li>
</ul>
<p><span class="caption-text">Benchmark</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/benchmark.html">性能数据</a></li>
</ul>
<p><span class="caption-text">API 文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/cxx_api_index.html">C++ API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/python_api_index.html">Python API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/c_api_index.html">C API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/go_api_index.html">GO API 文档</a></li>
</ul>
<p><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/faq.html">Paddle Inference FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/training_to_deployment.html">训练推理示例说明</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Paddle-Inference</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Using the Paddle-TensorRT Repository for Inference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/optimize/paddle_trt_en.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="using-the-paddle-tensorrt-repository-for-inference">
<h1>Using the Paddle-TensorRT Repository for Inference<a class="headerlink" href="#using-the-paddle-tensorrt-repository-for-inference" title="Permalink to this headline">¶</a></h1>
<p>NVIDIA TensorRT is an SDK for high-performance deep learning inference. It can lower the latency of the inference applications and improve their throughput. PaddlePaddle integrates TensorRT with subgraph design, so we can use the TensorRT module to enhance the performance of the Paddle model during the inference process. In this article, we will walk through how to use the subgraph module of Paddle-TRT to accelerate the inference.</p>
<p>If you need to install <a class="reference external" href="https://developer.nvidia.com/nvidia-tensorrt-6x-download">TensorRT</a>, please refer to the <a class="reference external" href="https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-601/tensorrt-install-guide/index.html">TensorRT document</a>.</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>After the model is loaded, the neural network can be represented as a computing graph consisting of variables and computing nodes. If the TensorRT subgraph mode is turned on, Paddle will analyze the computing graph, find out subgraphs that can be optimized by TensorRT there in the analysis, and replace them with TensorRT nodes. During inference, if encountering TensorRT nodes, Paddle will accelerate this node with TensorRT where other nodes were executed with the original implementation of Paddle. Besides the common optimization methods like the operator (OP) fusion, device memory optimization, TensorRT also contains the accelerated OP implementation to lower the inference latency and improve the throughput.</p>
<p>Currently, Paddle-TRT supports the static shape mode and the dynamic shape mode. Tasks like image classification, segmentation, and object detection are supported in the static mode. Inference acceleration under FP16 and Int8 are also supported. In the dynamic mode, in addition to the CV models (FCN, Faster R-CNN), NLP models (BERT, ERNIE, etc.) are also supported.</p>
<p><strong>Capabilities of Paddle-TRT：</strong></p>
<p><strong>1）Static shape：</strong></p>
<p>Supported models：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Classification</p></th>
<th class="head"><p>Detection</p></th>
<th class="head"><p>Segmentation</p></th>
</tr>
<tr class="row-even"><th class="head"><p>Models</p></th>
<th class="head"><p>Models</p></th>
<th class="head"><p>Models</p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><p>Mobilenetv1</p></td>
<td><p>yolov3</p></td>
<td><p>ICNET</p></td>
</tr>
<tr class="row-even"><td><p>Resnet50</p></td>
<td><p>SSD</p></td>
<td><p>UNet</p></td>
</tr>
<tr class="row-odd"><td><p>Vgg16</p></td>
<td><p>Mask-rcnn</p></td>
<td><p>FCN</p></td>
</tr>
<tr class="row-even"><td><p>Resnext</p></td>
<td><p>Faster-rcnn</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AlexNet</p></td>
<td><p>Cascade-rcnn</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Se-ResNext</p></td>
<td><p>Retinanet</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>GoogLeNet</p></td>
<td><p>Mobilenet-SSD</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>DPN</p></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Fp16: <input checked=""  type="checkbox"></p>
<p>Calib Int8: <input checked=""  type="checkbox"></p>
<p>Serialize optimized information: <input checked=""  type="checkbox"></p>
<p>Load the PaddleSlim Int8 model: <input checked=""  type="checkbox"></p>
<p><strong>2）Dynamic shape：</strong></p>
<p>Supported models：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Images</p></th>
<th class="head"><p>NLP</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FCN</p></td>
<td><p>Bert</p></td>
</tr>
<tr class="row-odd"><td><p>Faster_RCNN</p></td>
<td><p>Ernie</p></td>
</tr>
</tbody>
</table>
<p>Fp16: <input checked=""  type="checkbox"></p>
<p>Calib Int8: <input type="checkbox"></p>
<p>Serialize optimized information: <input type="checkbox"></p>
<p>Load the PaddleSlim Int8 model: <input type="checkbox"></p>
<p><strong>Note:</strong></p>
<ol class="arabic simple">
<li><p>During the compilation of the source code, the TensorRT inference repository only supports GPU compilation, and TENSORRT_ROOT is required to be set to the path of TensorRT.</p></li>
<li><p>Only TensorRT versions above 5.0 are supported by Windows.</p></li>
<li><p>The version of TRT  should be above 6.0 if the input of the dynamic shape uses Paddle-TRT.</p></li>
</ol>
</section>
<section id="i-environment-preparation">
<h2>I. Environment Preparation<a class="headerlink" href="#i-environment-preparation" title="Permalink to this headline">¶</a></h2>
<p>To use the functions of Paddle-TRT, the runtime environment of Paddle containing TRT is required. There are three ways to get prepared:</p>
<p>1）Using pip to install a whl file under linux</p>
<p>Download a whl file with the consistent environment and trt from <a class="reference external" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/install/Tables.html#whl-release">whl list</a>, and install it using pip.</p>
<p>2）Using the docker</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pull the docker, where the Paddle 2.2 Python environment has been preinstalled and there is a precompiled library (c++) put in the main directory ～/.</span>
docker pull paddlepaddle/paddle:latest-dev-cuda11.0-cudnn8-gcc82

sudo nvidia-docker run --name your_name -v <span class="nv">$PWD</span>:/paddle  --network<span class="o">=</span>host -it paddlepaddle/paddle:latest-dev-cuda11.0-cudnn8-gcc82  /bin/bash
</pre></div>
</div>
<p>3）Manual Compilation
Please refer to the <a class="reference external" href="../user_guides/source_compile.html">compilation document</a></p>
<p><strong>Note1：</strong> During the cmake, please set TENSORRT_ROOT （the path of TRT lib）and WITH_PYTHON （set “whether to produce the python whl file” to ON).</p>
<p><strong>Note2:</strong> There will be errors of TensorRT during the compilation.</p>
<p>Add virtual destructors to class IPluginFactory and class IGpuAllocator of NvInfer.h (trt5) or NvInferRuntime.h (trt6) file respectively by hand:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">virtual</span> <span class="o">~</span><span class="n">IPluginFactory</span><span class="p">()</span> <span class="p">{};</span>
<span class="k">virtual</span> <span class="o">~</span><span class="n">IGpuAllocator</span><span class="p">()</span> <span class="p">{};</span>
</pre></div>
</div>
<p>Change <strong>protected: ~IOptimizationProfile() noexcept = default;</strong> in <cite>NvInferRuntime.h</cite> (trt6)</p>
<p>to</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">virtual</span> <span class="o">~</span><span class="n">IOptimizationProfile</span><span class="p">()</span> <span class="k">noexcept</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="ii-introduction-to-the-usage-of-apis">
<h2>II. Introduction to the usage of APIs<a class="headerlink" href="#ii-introduction-to-the-usage-of-apis" title="Permalink to this headline">¶</a></h2>
<p>In the section of <a class="reference external" href="https://paddleinference.paddlepaddle.org.cn/quick_start/workflow.html">the inference process</a>, we have got to know that there are five parts of Paddle Inference:</p>
<ul class="simple">
<li><p>Configuration of inference options</p></li>
<li><p>Creation of the predictor</p></li>
<li><p>Preparation for the model input</p></li>
<li><p>Model inference</p></li>
<li><p>Acquisition of the model output</p></li>
</ul>
<p>Paddle-TRT also follows the same process. Let’s use a simple example to introduce it (It is assumed that you have known about the Paddle Inference). If you are new to this, you can visit &lt;<a class="reference external" href="https://paddleinference.paddlepaddle.org.cn/quick_start/workflow.html">https://paddleinference.paddlepaddle.org.cn/quick_start/workflow.html</a>&gt;`_ to get started.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">paddle.inference</span> <span class="k">as</span> <span class="nn">paddle_infer</span>

<span class="k">def</span> <span class="nf">create_predictor</span><span class="p">():</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">paddle_infer</span><span class="o">.</span><span class="n">Config</span><span class="p">(</span><span class="s2">&quot;./resnet50/model&quot;</span><span class="p">,</span> <span class="s2">&quot;./resnet50/params&quot;</span><span class="p">)</span>
    <span class="n">config</span><span class="o">.</span><span class="n">enable_memory_optim</span><span class="p">()</span>
    <span class="n">config</span><span class="o">.</span><span class="n">enable_use_gpu</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Open TensorRT. The details of this interface will be mentioned in the following part.</span>
    <span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span><span class="n">workspace_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span><span class="p">,</span>
                                  <span class="n">max_batch_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                                  <span class="n">min_subgraph_size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                                  <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Float32</span><span class="p">,</span>
                                  <span class="n">use_static</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">predictor</span> <span class="o">=</span> <span class="n">paddle_infer</span><span class="o">.</span><span class="n">create_predictor</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">predictor</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">predictor</span><span class="p">,</span> <span class="n">img</span><span class="p">):</span>
    <span class="c1"># Preparation for the input</span>
    <span class="n">input_names</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">get_input_names</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span>  <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_names</span><span class="p">):</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">get_input_handle</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">copy_from_cpu</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
    <span class="c1"># Inference</span>
    <span class="n">predictor</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Acquisition of the output</span>
    <span class="n">output_names</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">get_output_names</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_names</span><span class="p">):</span>
        <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">get_output_handle</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">output_data</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="o">.</span><span class="n">copy_to_cpu</span><span class="p">()</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">create_predictor</span><span class="p">()</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="p">[</span><span class="n">img</span><span class="p">])</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;class index: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
<p>From this example, it is clear that we open TensorRT options through the interface of <cite>enable_tensorrt_engine</cite>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span><span class="n">workspace_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span><span class="p">,</span>
                              <span class="n">max_batch_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                              <span class="n">min_subgraph_size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                              <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Float32</span><span class="p">,</span>
                              <span class="n">use_static</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, let’s have a look at the function of each parameter in the interface:</p>
<ul class="simple">
<li><p><strong>workspace_size</strong>，type：int，and the default value is 1 &lt;&lt; 30 （1G）. It designates the size of the working space of TensorRT, and TensorRT will sort out the optimum kernel for the execution of the inference computation under this limitation.</p></li>
<li><p><strong>max_batch_size</strong>，type：int，and the default value is 1. The maximum batch is required to be set beforehand, and the batch size cannot exceed this max value in the execution.</p></li>
<li><p><strong>min_subgraph_size</strong>，type：int，and the default value is 3. Paddle-TRT is operated in subgraphs. In order to avoid performance loss, Paddle-TRT will be operated only when the number of nodes within subgraphs is more than min_subgraph_size.</p></li>
<li><p><strong>precision_mode</strong>，type: <strong>paddle_infer.PrecisionType</strong>, and the default value is <strong>paddle_infer.PrecisionType.Float32</strong>. It designates the precision of TRT, and supports FP32（Float32）,FP16（Half）,and Int8（Int8）. If you need to use the post-training quantization (PTQ, or offline quantization) calibration of Paddle-TRT int8, set the precision to <strong>paddle_infer.PrecisionType.Int8</strong> and <strong>use_calib_mode</strong> to True.</p></li>
<li><p><strong>use_static</strong>，type：bool, and the default value is False. If it is designated as True, then the optimized TRT information will be serialized to the disk during the first run of the program, and will be directly loaded next time without regeneration.</p></li>
<li><p><strong>use_calib_mode</strong>，type：bool, and the default value is False. If you need to use the PTQ calibration of Paddle-TRT int8, set this to True.</p></li>
</ul>
<section id="int8-quantization-inference">
<h3>Int8 Quantization Inference<a class="headerlink" href="#int8-quantization-inference" title="Permalink to this headline">¶</a></h3>
<p>To some extent, the parameters of the neural network are redundant. And in many tasks, we can turn the Float32 model into the Int8 model with the cost of an acceptable precision loss, in order to reduce the computation amount, computation time, memory used, and the model size. There are two steps to use Int8 for quantized inference: 1) produce the quantized model; 2) load the quantized model for Int8 inference. In the following part, we will elaborate on how to use Paddle-TRT for Int8 quantized inference.</p>
<p><strong>1. Produce the quantized model</strong></p>
<p>There are two methods are supported currently:</p>
<ol class="loweralpha simple">
<li><p>Use the built-in functionality of TensorRT– Int8 PTQ calibration. In calibration, a calibration table is made based on the trained FP32 model and a few calibrated data (e.g. about 500-1000 images), and during the inference, the FP32 model and the table can be used for the Int8 precision inference. Follow the guide to make the calibration table:</p></li>
</ol>
<blockquote>
<div><ul>
<li><p>When configurating TensorRT，set <strong>precision_mode</strong> to <strong>paddle_infer.PrecisionType.Int8</strong> and <strong>use_calib_mode</strong> to <strong>True</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span>
  <span class="n">workspace_size</span><span class="o">=</span><span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">30</span><span class="p">,</span>
  <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_subgraph_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Int8</span><span class="p">,</span>
  <span class="n">use_static</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Prepare about 500 real input images, and run the model with the above configuration. (Paddle-TRT counts the range value of every tensor and records it in the table. After the running, the table will be written into <cite>_opt_cache</cite>.</p></li>
</ul>
<p>If you want to know the code of making the calibration table using TensorRT’s built-in functionality of Int8 PTQ calibration, please refer to <a class="reference external" href="https://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/c%2B%2B/paddle-trt/README.md#%E7%94%9F%E6%88%90%E9%87%8F%E5%8C%96%E6%A0%A1%E5%87%86%E8%A1%A8">the demo here</a> .</p>
</div></blockquote>
<ol class="loweralpha simple" start="2">
<li><p>Use the model compression tool library– PaddleSlim to make the quantized model. PaddleSlim supports offline quantization and online quantization. And the offline quantization is similar to TensorRT PTQ calibration in principle; online quantization is also called quantization aware training (QAT), which depends on massive data (e.g. &gt;=5000 images) to retrain the pretrained model and uses quantization simulation to update the weight in the training so that errors can be reduced. If you want to learn about how to make the quantized model using PaddleSlim, please refer to:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Post-training quantization <a class="reference external" href="https://paddlepaddle.github.io/PaddleSlim/quick_start/quant_post_tutorial.html">quick start</a></p></li>
<li><p>Post-training quantization <a class="reference external" href="https://paddlepaddle.github.io/PaddleSlim/api_cn/quantization_api.html#quant-post">API description</a></p></li>
<li><p>Post-training quantization <a class="reference external" href="https://github.com/PaddlePaddle/PaddleSlim/tree/release/1.1.0/demo/quant/quant_post">Demo</a></p></li>
<li><p>Quant aware training <a class="reference external" href="https://paddlepaddle.github.io/PaddleSlim/quick_start/quant_aware_tutorial.html">quick start</a></p></li>
<li><p>Quant aware training <a class="reference external" href="https://paddlepaddle.github.io/PaddleSlim/api_cn/quantization_api.html#quant-aware">API description</a></p></li>
<li><p>Quant aware training <a class="reference external" href="https://github.com/PaddlePaddle/PaddleSlim/tree/release/1.1.0/demo/quant/quant_aware">Demo</a></p></li>
</ul>
</div></blockquote>
<p>In PTQ, retraining is not required, but the precision may be affected. In QAT, the precision may be less affected, but retraining is required, and it is more complicated to perform QAT. Practically speaking, it is recommended to use the TRT functionality of PTQ calibration to make the quantized model. If the precision cannot meet the standard, then resort to PaddleSlim.</p>
<p><strong>2. Load the quantized model for Int8 inference</strong></p>
<blockquote>
<div><p>First, in the configuration of TensorRT, set <strong>precision_mode</strong> to <strong>paddle_infer.PrecisionType.Int8</strong> .</p>
<p>If the quantized model is made by the TRT PTQ calibration, set <strong>use_calib_mode</strong> to <strong>True</strong> ：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span>
  <span class="n">workspace_size</span><span class="o">=</span><span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">30</span><span class="p">,</span>
  <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_subgraph_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Int8</span><span class="p">,</span>
  <span class="n">use_static</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>For the complete demo, please refer to <a class="reference external" href="https://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/c%2B%2B/paddle-trt/README.md#%E5%8A%A0%E8%BD%BD%E6%A0%A1%E5%87%86%E8%A1%A8%E6%89%A7%E8%A1%8Cint8%E9%A2%84%E6%B5%8B">here</a>.</p>
<p>If the quantized model is made by PaddleSlim quantization，set <strong>use_calib_mode</strong> to <strong>False</strong> ：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span>
  <span class="n">workspace_size</span><span class="o">=</span><span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">30</span><span class="p">,</span>
  <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_subgraph_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Int8</span><span class="p">,</span>
  <span class="n">use_static</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>For the complete demo, please refer to <a class="reference external" href="https://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/c%2B%2B/paddle-trt/README.md#%E4%B8%89%E4%BD%BF%E7%94%A8trt-%E5%8A%A0%E8%BD%BDpaddleslim-int8%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B">here</a> .</p>
</div></blockquote>
</section>
<section id="run-dynamic-shape">
<h3>Run dynamic shape<a class="headerlink" href="#run-dynamic-shape" title="Permalink to this headline">¶</a></h3>
<p>Since version 1.8, Paddle has begun to support the dynamic shape for the TRT subgraph.
APIs adopted here include：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">enable_tensorrt_engine</span><span class="p">(</span>
  <span class="n">workspace_size</span> <span class="o">=</span> <span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">30</span><span class="p">,</span>
  <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_subgraph_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
  <span class="n">precision_mode</span><span class="o">=</span><span class="n">paddle_infer</span><span class="o">.</span><span class="n">PrecisionType</span><span class="o">.</span><span class="n">Float32</span><span class="p">,</span>
  <span class="n">use_static</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_calib_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">min_input_shape</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]}</span>
<span class="n">max_input_shape</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">]}</span>
<span class="n">opt_input_shape</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">]}</span>

<span class="n">config</span><span class="o">.</span><span class="n">set_trt_dynamic_shape_info</span><span class="p">(</span><span class="n">min_input_shape</span><span class="p">,</span> <span class="n">max_input_shape</span><span class="p">,</span> <span class="n">opt_input_shape</span><span class="p">)</span>
</pre></div>
</div>
<p>It can be seen that on the basis of config.enable_tensorrt_engine，there is another interface–config.set_trt_dynamic_shape_info added.</p>
<p>The newly added interface is used to set the minimum, maximum, and optimum input shapes. The optimum shape lies between the minimum and the maximum. At the beginning of the inference, the optimum kernel of OPs will be chosen according to the optimum shape.</p>
<p>The <strong>config.set_trt_dynamic_shape_info</strong> interface is adopted, and the predictor will run the dynamic input mode of the TRT subgraph. During the running, any input shape between the minimum and the maximum is OK.</p>
</section>
</section>
<section id="iii-test-demo">
<h2>III. Test demo<a class="headerlink" href="#iii-test-demo" title="Permalink to this headline">¶</a></h2>
<p>More demos using the TRT subgraph for inference are provided on the github.</p>
<ul class="simple">
<li><p>For Python demos, please refer to <a class="reference external" href="https://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/python/paddle_trt">the link</a> .</p></li>
<li><p>For C++ demos, please refer to <a class="reference external" href="https://github.com/PaddlePaddle/Paddle-Inference-Demo/tree/master/c%2B%2B/paddle-trt">the link</a> .</p></li>
</ul>
</section>
<section id="iv-the-principle-of-the-paddle-trt-subgraph">
<h2>IV. The principle of the Paddle-TRT subgraph<a class="headerlink" href="#iv-the-principle-of-the-paddle-trt-subgraph" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>PaddlePaddle uses the subgraph to integrate TensorRT, and after loading the model, the neural network can be presented as a computing chart consisting of variables and computing nodes. Paddle TensorRT scans the whole image, detects subgraphs which can be optimized by TensorRT, and replaces them with its nodes. If encountering TensorRT nodes, Paddle will adopt the TensorRT repository to optimize them and use its original implementation for other nodes. During the inference, TensorRT can merge OPs both horizontally and vertically, filter out redundant OPs, and choose optimum kernels to optimize OPs in certain platforms so that the model inference can be accelerated.</p>
</div></blockquote>
<p>The following figure shows the process by taking a simple model as an example:</p>
<p><strong>Original Network</strong></p>
<blockquote>
<div><img alt="https://raw.githubusercontent.com/NHZlX/FluidDoc/add_trt_doc/doc/fluid/user_guides/howto/inference/image/model_graph_original.png" src="https://raw.githubusercontent.com/NHZlX/FluidDoc/add_trt_doc/doc/fluid/user_guides/howto/inference/image/model_graph_original.png" />
</div></blockquote>
<p><strong>Converted Network</strong></p>
<blockquote>
<div><blockquote>
<div><img alt="https://raw.githubusercontent.com/NHZlX/FluidDoc/add_trt_doc/doc/fluid/user_guides/howto/inference/image/model_graph_trt.png" src="https://raw.githubusercontent.com/NHZlX/FluidDoc/add_trt_doc/doc/fluid/user_guides/howto/inference/image/model_graph_trt.png" />
</div></blockquote>
<p>From the original network, we can know that the green nodes are those supported by TensorRT, that the red ones are variables in the network, and that the yellow ones are the nodes that only can be executed by Paddle’s original implementation. Those green nodes are extracted from the original network and integrated into subgraphs. Then they are replaced with a TensorRT node and turn into the <strong>block-25</strong> node. When meeting this node, Paddle will call the TensorRT repository to execute it.</p>
</div></blockquote>
</section>
</section>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Paddle-Inference Developer

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>