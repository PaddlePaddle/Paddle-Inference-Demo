

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>4. 使用 GPU 进行预测 &mdash; Paddle-Inference  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="5. 使用 XPU 进行预测" href="XPUConfig.html" />
    <link rel="prev" title="3. 使用 CPU 进行预测" href="CPUConfig.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> Paddle-Inference
          

          
          </a>

          
            
            
              <div class="version">
                latest
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p><span class="caption-text">产品介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../product_introduction/summary.html">飞桨推理产品简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../product_introduction/inference_intro.html">Paddle Inference 简介</a></li>
</ul>
<p><span class="caption-text">快速开始</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start/workflow.html">预测流程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start/cpp_demo.html">预测示例 (C++)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start/python_demo.html">预测示例 (Python)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start/c_demo.html">预测示例 (C)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start/go_demo.html">预测示例 (GO)</a></li>
</ul>
<p><span class="caption-text">使用方法</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guides/source_compile.html">源码编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guides/compile_ARM.html"><strong>飞腾/鲲鹏下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guides/compile_SW.html"><strong>申威下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guides/compile_ZHAOXIN.html"><strong>兆芯下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guides/compile_MIPS.html"><strong>龙芯下从源码编译</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guides/download_lib.html">下载安装Linux预测库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guides/download_lib.html#windows">下载安装Windows预测库</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guides/download_lib.html#mac">下载安装Mac预测库</a></li>
</ul>
<p><span class="caption-text">性能调优</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../optimize/paddle_x86_cpu_int8.html">X86 CPU 上部署量化模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optimize/paddle_x86_cpu_bf16.html">X86 CPU 上部署BF16预测</a></li>
</ul>
<p><span class="caption-text">工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tools/visual.html">模型可视化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tools/x2paddle.html">模型转换工具 X2Paddle</a></li>
</ul>
<p><span class="caption-text">硬件部署示例</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../demo_tutorial/x86_linux_demo.html">X86 Linux上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../demo_tutorial/x86_windows_demo.html">X86 Windows上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../demo_tutorial/paddle_xpu_infer_cn.html">使用昆仑预测</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../demo_tutorial/cuda_linux_demo.html">Linux上GPU预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../demo_tutorial/cuda_jetson_demo.html">NV Jetson上预测部署示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../demo_tutorial/cuda_windows_demo.html">Windows上GPU预测部署示例</a></li>
</ul>
<p><span class="caption-text">Benchmark</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark/benchmark.html">性能数据</a></li>
</ul>
<p><span class="caption-text">API 文档</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../cxx_api_index.html">C++ API 文档</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../CreatePredictor.html">CreatePredictor 方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../CreatePredictor.html#getversion">GetVersion 方法</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../Config_index.html">Config 类</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ConfigClass.html">1. Config 构造函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="ModelConfig.html">2. 设置预测模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="CPUConfig.html">3. 使用 CPU 进行预测</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">4. 使用 GPU 进行预测</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#gpu-1">4.1. GPU 设置</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cudnn">4.2. CUDNN 设置</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tensorrt">4.3. TensorRT 设置</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="XPUConfig.html">5. 使用 XPU 进行预测</a></li>
<li class="toctree-l3"><a class="reference internal" href="ORTConfig.html">6. 使用 ONNXRuntime 进行预测</a></li>
<li class="toctree-l3"><a class="reference internal" href="IPUConfig.html">7. 使用 IPU 进行预测</a></li>
<li class="toctree-l3"><a class="reference internal" href="OptimConfig.html">8. 设置模型优化方法</a></li>
<li class="toctree-l3"><a class="reference internal" href="OtherFunction.html">9. 启用内存优化</a></li>
<li class="toctree-l3"><a class="reference internal" href="OtherFunction.html#section-2">10. 设置缓存路径</a></li>
<li class="toctree-l3"><a class="reference internal" href="OtherFunction.html#fc-padding">11. FC Padding</a></li>
<li class="toctree-l3"><a class="reference internal" href="OtherFunction.html#profile">12. Profile 设置</a></li>
<li class="toctree-l3"><a class="reference internal" href="OtherFunction.html#log">13. Log 设置</a></li>
<li class="toctree-l3"><a class="reference internal" href="OtherFunction.html#config">14. 查看config配置</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../PaddlePassBuilder.html">PaddlePassBuilder 类</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Predictor.html">Predictor 类</a></li>
<li class="toctree-l2"><a class="reference internal" href="../PredictorPool.html">PredictorPool 类</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Tensor.html">Tensor 类</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Enum.html">枚举类型</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../python_api_index.html">Python API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../c_api_index.html">C API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../go_api_index.html">GO API 文档</a></li>
</ul>
<p><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/faq.html">Paddle Inference FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/training_to_deployment.html">训练推理示例说明</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Paddle-Inference</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../cxx_api_index.html">C++ API 文档</a> &raquo;</li>
        
          <li><a href="../Config_index.html">Config 类</a> &raquo;</li>
        
      <li><span class="section-number">4. </span>使用 GPU 进行预测</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/api_reference/cxx_api_doc/Config/GPUConfig.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="gpu">
<h1><span class="section-number">4. </span>使用 GPU 进行预测<a class="headerlink" href="#gpu" title="Permalink to this headline">¶</a></h1>
<p><strong>注意：</strong></p>
<ol class="simple">
<li><p>Config 默认使用 CPU 进行预测，需要通过 <code class="docutils literal notranslate"><span class="pre">EnableUseGpu</span></code> 来启用 GPU 预测</p></li>
<li><p>可以尝试启用 CUDNN 和 TensorRT 进行 GPU 预测加速</p></li>
</ol>
<section id="gpu-1">
<h2><span class="section-number">4.1. </span>GPU 设置<a class="headerlink" href="#gpu-1" title="Permalink to this headline">¶</a></h2>
<p>API定义如下：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// 启用 GPU 进行预测</span>
<span class="c1">// 参数：memory_pool_init_size_mb - 初始化分配的gpu显存，以MB为单位</span>
<span class="c1">//      device_id - 设备id</span>
<span class="c1">// 返回：None</span>
<span class="kt">void</span> <span class="nf">EnableUseGpu</span><span class="p">(</span><span class="kt">uint64_t</span> <span class="n">memory_pool_init_size_mb</span><span class="p">,</span> <span class="kt">int</span> <span class="n">device_id</span> <span class="o">=</span> <span class="mi">0</span><span class="p">);</span>

<span class="c1">// 禁用 GPU 进行预测</span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：None</span>
<span class="kt">void</span> <span class="nf">DisableGpu</span><span class="p">();</span>

<span class="c1">// 判断是否启用 GPU </span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：bool - 是否启用 GPU </span>
<span class="kt">bool</span> <span class="nf">use_gpu</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>

<span class="c1">// 获取 GPU 的device id</span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：int -  GPU 的device id</span>
<span class="kt">int</span> <span class="nf">gpu_device_id</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>

<span class="c1">// 获取 GPU 的初始显存大小</span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：int -  GPU 的初始的显存大小</span>
<span class="kt">int</span> <span class="nf">memory_pool_init_size_mb</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>

<span class="c1">// 初始化显存占总显存的百分比</span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：float - 初始的显存占总显存的百分比</span>
<span class="kt">float</span> <span class="nf">fraction_of_gpu_memory_for_pool</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>

<span class="c1">// 开启线程流，目前的行为是为每一个线程绑定一个流，在将来该行为可能改变</span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：None</span>
<span class="kt">void</span> <span class="nf">EnableGpuMultiStream</span><span class="p">();</span>

<span class="c1">// 判断是否开启线程流</span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：bool - 是否是否开启线程流</span>
<span class="kt">bool</span> <span class="nf">thread_local_stream_enabled</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>

<span class="c1">// 启用 GPU FP16 计算精度进行预测</span>
<span class="c1">// 参数：op_list - 保持 FP32 计算精度算子名单</span>
<span class="c1">// 返回：None</span>
<span class="kt">void</span> <span class="nf">Exp_EnableUseGpuFp16</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">op_list</span><span class="p">);</span>

<span class="c1">// 判断是否启用 GPU FP16 计算精度 </span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：bool - 是否启用 GPU FP16 计算精度</span>
<span class="kt">bool</span> <span class="nf">gpu_fp16_enabled</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>
</pre></div>
</div>
<p>GPU设置代码示例：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// 创建默认 Config 对象</span>
<span class="n">paddle_infer</span><span class="o">::</span><span class="n">Config</span> <span class="n">config</span><span class="p">;</span>

<span class="c1">// 启用 GPU 进行预测 - 初始化 GPU 显存 100M, Deivce_ID 为 0</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableUseGpu</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="c1">// 通过 API 获取 GPU 信息</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Use GPU is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">use_gpu</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span> <span class="c1">// true</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Init mem size is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">memory_pool_init_size_mb</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Init mem frac is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">fraction_of_gpu_memory_for_pool</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;GPU device id is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">gpu_device_id</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="c1">// 禁用 GPU 进行预测</span>
<span class="n">config</span><span class="p">.</span><span class="n">DisableGpu</span><span class="p">();</span>
<span class="c1">// 通过 API 获取 GPU 信息</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Use GPU is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">use_gpu</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span> <span class="c1">// false</span>

<span class="c1">// 启用 GPU FP16 计算精度进行预测</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableUseGpu</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="n">config</span><span class="p">.</span><span class="n">Exp_EnableUseGpuFp16</span><span class="p">();</span>
<span class="c1">// 通过 API 获取是否启用了 GPU FP16 计算精度</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Use GPU FP16 is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">gpu_fp16_enabled</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span> <span class="c1">// true</span>
</pre></div>
</div>
<p>开启多线程流代码示例：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// 自定义 Barrier 类，用于线程间同步</span>
<span class="k">class</span> <span class="nc">Barrier</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="k">explicit</span> <span class="n">Barrier</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">count</span><span class="p">)</span> <span class="o">:</span> <span class="n">_count</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="p">{}</span>
  <span class="kt">void</span> <span class="n">Wait</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">_mutex</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">--</span><span class="n">_count</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">_cv</span><span class="p">.</span><span class="n">wait</span><span class="p">(</span><span class="n">lock</span><span class="p">,</span> <span class="p">[</span><span class="k">this</span><span class="p">]</span> <span class="p">{</span> <span class="k">return</span> <span class="n">_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span> <span class="p">});</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">_cv</span><span class="p">.</span><span class="n">notify_all</span><span class="p">();</span>
    <span class="p">}</span>
  <span class="p">}</span>
 <span class="k">private</span><span class="o">:</span>
  <span class="n">std</span><span class="o">::</span><span class="n">mutex</span> <span class="n">_mutex</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">condition_variable</span> <span class="n">_cv</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">_count</span><span class="p">;</span>
<span class="p">};</span>

<span class="kt">int</span> <span class="nf">test_main</span><span class="p">(</span><span class="k">const</span> <span class="n">paddle_infer</span><span class="o">::</span><span class="n">Config</span><span class="o">&amp;</span> <span class="n">config</span><span class="p">,</span> <span class="n">Barrier</span><span class="o">*</span> <span class="n">barrier</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">static</span> <span class="n">std</span><span class="o">::</span><span class="n">mutex</span> <span class="n">mutex</span><span class="p">;</span>
  <span class="c1">// 创建 Predictor 对象</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">paddle_infer</span><span class="o">::</span><span class="n">Predictor</span><span class="o">&gt;</span> <span class="n">predictor</span><span class="p">;</span>
  <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">mutex</span><span class="p">);</span>
    <span class="n">predictor</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">paddle_infer</span><span class="o">::</span><span class="n">CreatePredictor</span><span class="p">(</span><span class="n">config</span><span class="p">));</span>
  <span class="p">}</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">barrier</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">barrier</span><span class="o">-&gt;</span><span class="n">Wait</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="c1">// 准备输入数据</span>
  <span class="kt">int</span> <span class="n">input_num</span> <span class="o">=</span> <span class="n">shape_production</span><span class="p">(</span><span class="n">INPUT_SHAPE</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">input_data</span><span class="p">(</span><span class="n">input_num</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
  <span class="k">auto</span> <span class="n">input_names</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">-&gt;</span><span class="n">GetInputNames</span><span class="p">();</span>
  <span class="k">auto</span> <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">-&gt;</span><span class="n">GetInputHandle</span><span class="p">(</span><span class="n">input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
  <span class="n">input_tensor</span><span class="o">-&gt;</span><span class="n">Reshape</span><span class="p">(</span><span class="n">INPUT_SHAPE</span><span class="p">);</span>
  <span class="n">input_tensor</span><span class="o">-&gt;</span><span class="n">CopyFromCpu</span><span class="p">(</span><span class="n">input_data</span><span class="p">.</span><span class="n">data</span><span class="p">());</span>
  <span class="c1">// 执行预测</span>
  <span class="n">predictor</span><span class="o">-&gt;</span><span class="n">Run</span><span class="p">();</span>
  <span class="c1">// 获取预测输出</span>
  <span class="k">auto</span> <span class="n">output_names</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">-&gt;</span><span class="n">GetOutputNames</span><span class="p">();</span>
  <span class="k">auto</span> <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">-&gt;</span><span class="n">GetOutputHandle</span><span class="p">(</span><span class="n">output_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">output_shape</span> <span class="o">=</span> <span class="n">output_tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">();</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Output shape is &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">shape_to_string</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">**</span><span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="kt">size_t</span> <span class="n">thread_num</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="o">&gt;</span> <span class="n">threads</span><span class="p">(</span><span class="n">thread_num</span><span class="p">);</span>
  <span class="n">Barrier</span> <span class="n">barrier</span><span class="p">(</span><span class="n">thread_num</span><span class="p">);</span>
  <span class="c1">// 创建 5 个线程，并为每个线程开启一个单独的GPU Stream</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">threads</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">threads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">([</span><span class="o">&amp;</span><span class="n">barrier</span><span class="p">,</span> <span class="n">i</span><span class="p">]()</span> <span class="p">{</span>
      <span class="n">paddle_infer</span><span class="o">::</span><span class="n">Config</span> <span class="n">config</span><span class="p">;</span>
      <span class="n">config</span><span class="p">.</span><span class="n">EnableUseGpu</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
      <span class="n">config</span><span class="p">.</span><span class="n">SetModel</span><span class="p">(</span><span class="n">FLAGS_infer_model</span><span class="p">);</span>
      <span class="n">config</span><span class="p">.</span><span class="n">EnableGpuMultiStream</span><span class="p">();</span>
      <span class="n">test_main</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">barrier</span><span class="p">);</span>
    <span class="p">});</span>
  <span class="p">}</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="nl">th</span> <span class="p">:</span> <span class="n">threads</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">th</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="cudnn">
<h2><span class="section-number">4.2. </span>CUDNN 设置<a class="headerlink" href="#cudnn" title="Permalink to this headline">¶</a></h2>
<p><strong>注意：</strong> 启用 CUDNN 的前提为已经启用 GPU，否则启用 CUDNN 无法生效。</p>
<p>API定义如下：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// 启用 CUDNN 进行预测加速</span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：None</span>
<span class="kt">void</span> <span class="nf">EnableCUDNN</span><span class="p">();</span>

<span class="c1">// 判断是否启用 CUDNN </span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：bool - 是否启用 CUDNN</span>
<span class="kt">bool</span> <span class="nf">cudnn_enabled</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>
</pre></div>
</div>
<p>代码示例：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// 创建默认 Config 对象</span>
<span class="n">paddle_infer</span><span class="o">::</span><span class="n">Config</span> <span class="n">config</span><span class="p">();</span>

<span class="c1">// 启用 GPU 进行预测</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableUseGpu</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="c1">// 启用 CUDNN 进行预测加速</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableCUDNN</span><span class="p">();</span>
<span class="c1">// 通过 API 获取 CUDNN 启用结果</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Enable CUDNN is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">cudnn_enabled</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span> <span class="c1">// true</span>

<span class="c1">// 禁用 GPU 进行预测</span>
<span class="n">config</span><span class="p">.</span><span class="n">DisableGpu</span><span class="p">();</span>
<span class="c1">// 启用 CUDNN 进行预测加速 - 因为 GPU 被禁用，因此 CUDNN 启用不生效</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableCUDNN</span><span class="p">();</span>
<span class="c1">// 通过 API 获取 CUDNN 启用结果</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Enable CUDNN is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">cudnn_enabled</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span> <span class="c1">// false</span>
</pre></div>
</div>
</section>
<section id="tensorrt">
<h2><span class="section-number">4.3. </span>TensorRT 设置<a class="headerlink" href="#tensorrt" title="Permalink to this headline">¶</a></h2>
<p><strong>注意：</strong></p>
<ol class="simple">
<li><p>启用 TensorRT 的前提为已经启用 GPU，否则启用 TensorRT 无法生效</p></li>
<li><p>对存在LoD信息的模型，如Bert, Ernie等NLP模型，必须使用动态 Shape</p></li>
<li><p>启用 TensorRT OSS 可以支持更多 plugin，详细参考 <a class="reference external" href="https://news.developer.nvidia.com/nvidia-open-sources-parsers-and-plugins-in-tensorrt/">TensorRT OSS</a></p></li>
</ol>
<p>更多 TensorRT 详细信息，请参考 <a class="reference external" href="../../../optimize/paddle_trt">使用Paddle-TensorRT库预测</a>。</p>
<p>API定义如下：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// 启用 TensorRT 进行预测加速</span>
<span class="c1">// 参数：workspace_size     - 指定 TensorRT 使用的工作空间大小</span>
<span class="c1">//      max_batch_size     - 设置最大的 batch 大小，运行时 batch 大小不得超过此限定值</span>
<span class="c1">//      min_subgraph_size  - Paddle-TRT 是以子图的形式运行，为了避免性能损失，当子图内部节点个数</span>
<span class="c1">//                           大于 min_subgraph_size 的时候，才会使用 Paddle-TRT 运行</span>
<span class="c1">//      precision          - 指定使用 TRT 的精度，支持 FP32(kFloat32)，FP16(kHalf)，Int8(kInt8)</span>
<span class="c1">//      use_static         - 若指定为 true，在初次运行程序的时候会将 TRT 的优化信息进行序列化到磁盘上，</span>
<span class="c1">//                           下次运行时直接加载优化的序列化信息而不需要重新生成</span>
<span class="c1">//      use_calib_mode     - 若要运行 Paddle-TRT INT8 离线量化校准，需要将此选项设置为 true</span>
<span class="c1">// 返回：None</span>
<span class="kt">void</span> <span class="nf">EnableTensorRtEngine</span><span class="p">(</span><span class="kt">int</span> <span class="n">workspace_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">20</span><span class="p">,</span>
                          <span class="kt">int</span> <span class="n">max_batch_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="kt">int</span> <span class="n">min_subgraph_size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                          <span class="n">Precision</span> <span class="n">precision</span> <span class="o">=</span> <span class="n">Precision</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">,</span>
                          <span class="kt">bool</span> <span class="n">use_static</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span>
                          <span class="kt">bool</span> <span class="n">use_calib_mode</span> <span class="o">=</span> <span class="nb">true</span><span class="p">);</span>
<span class="c1">// 判断是否启用 TensorRT </span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：bool - 是否启用 TensorRT</span>
<span class="kt">bool</span> <span class="nf">tensorrt_engine_enabled</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>

<span class="c1">// 设置 TensorRT 的动态 Shape</span>
<span class="c1">// 参数：min_input_shape          - TensorRT 子图支持动态 shape 的最小 shape</span>
<span class="c1">//      max_input_shape          - TensorRT 子图支持动态 shape 的最大 shape</span>
<span class="c1">//      optim_input_shape        - TensorRT 子图支持动态 shape 的最优 shape</span>
<span class="c1">//      disable_trt_plugin_fp16  - 设置 TensorRT 的 plugin 不在 fp16 精度下运行</span>
<span class="c1">// 返回：None</span>
<span class="kt">void</span> <span class="nf">SetTRTDynamicShapeInfo</span><span class="p">(</span>
      <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span> <span class="n">min_input_shape</span><span class="p">,</span>
      <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span> <span class="n">max_input_shape</span><span class="p">,</span>
      <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span> <span class="n">optim_input_shape</span><span class="p">,</span>
      <span class="kt">bool</span> <span class="n">disable_trt_plugin_fp16</span> <span class="o">=</span> <span class="nb">false</span><span class="p">);</span>

<span class="c1">// 启用 TensorRT OSS 进行预测加速</span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：None</span>
<span class="kt">void</span> <span class="nf">EnableTensorRtOSS</span><span class="p">();</span>

<span class="c1">// 判断是否启用 TensorRT OSS</span>
<span class="c1">// 参数：None</span>
<span class="c1">// 返回：bool - 是否启用 TensorRT OSS</span>
<span class="kt">bool</span> <span class="nf">tensorrt_oss_enabled</span><span class="p">();</span>

<span class="c1">/// 启用TensorRT DLA进行预测加速</span>
<span class="c1">/// 参数：dla_core - DLA设备的id，可选0，1，...，DLA设备总数 - 1</span>
<span class="c1">/// 返回：None</span>
<span class="kt">void</span> <span class="nf">EnableTensorRtDLA</span><span class="p">(</span><span class="kt">int</span> <span class="n">dla_core</span> <span class="o">=</span> <span class="mi">0</span><span class="p">);</span>

<span class="c1">/// 判断是否已经开启TensorRT DLA加速</span>
<span class="c1">/// 参数：None</span>
<span class="c1">/// 返回：bool - 是否已开启TensorRT DLA加速</span>
<span class="kt">bool</span> <span class="nf">tensorrt_dla_enabled</span><span class="p">();</span>
</pre></div>
</div>
<p>代码示例 (1)：使用 TensorRT FP32 / FP16 / INT8 进行预测</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// 创建 Config 对象</span>
<span class="n">paddle_infer</span><span class="o">::</span><span class="n">Config</span> <span class="n">config</span><span class="p">(</span><span class="n">FLAGS_infer_model</span> <span class="o">+</span> <span class="s">&quot;/mobilenet&quot;</span><span class="p">);</span>

<span class="c1">// 启用 GPU 进行预测</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableUseGpu</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

<span class="c1">// 启用 TensorRT 进行预测加速 - FP32</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableTensorRtEngine</span><span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> 
                            <span class="n">paddle_infer</span><span class="o">::</span><span class="n">PrecisionType</span><span class="o">::</span><span class="n">kFloat32</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="nb">false</span><span class="p">);</span>
<span class="c1">// 通过 API 获取 TensorRT 启用结果 - true</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Enable TensorRT is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">tensorrt_engine_enabled</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="c1">// 启用 TensorRT 进行预测加速 - FP16</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableTensorRtEngine</span><span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> 
                            <span class="n">paddle_infer</span><span class="o">::</span><span class="n">PrecisionType</span><span class="o">::</span><span class="n">kHalf</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="nb">false</span><span class="p">);</span>
<span class="c1">// 通过 API 获取 TensorRT 启用结果 - true</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Enable TensorRT is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">tensorrt_engine_enabled</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="c1">// 启用 TensorRT 进行预测加速 - Int8</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableTensorRtEngine</span><span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> 
                            <span class="n">paddle_infer</span><span class="o">::</span><span class="n">PrecisionType</span><span class="o">::</span><span class="n">kInt8</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="nb">true</span><span class="p">);</span>
<span class="c1">// 通过 API 获取 TensorRT 启用结果 - true</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Enable TensorRT is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">tensorrt_engine_enabled</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>代码示例 (2)：使用 TensorRT 动态 Shape 进行预测</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// 创建 Config 对象</span>
<span class="n">paddle_infer</span><span class="o">::</span><span class="n">Config</span> <span class="n">config</span><span class="p">(</span><span class="n">FLAGS_infer_model</span> <span class="o">+</span> <span class="s">&quot;/mobilenet&quot;</span><span class="p">);</span>

<span class="c1">// 启用 GPU 进行预测</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableUseGpu</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

<span class="c1">// 启用 TensorRT 进行预测加速 - Int8</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableTensorRtEngine</span><span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="n">paddle_infer</span><span class="o">::</span><span class="n">PrecisionType</span><span class="o">::</span><span class="n">kInt8</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="nb">true</span><span class="p">);</span>
<span class="c1">// 设置模型输入的动态 Shape 范围</span>
<span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span> <span class="n">min_input_shape</span> <span class="o">=</span> <span class="p">{{</span><span class="s">&quot;image&quot;</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">}}};</span>
<span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span> <span class="n">max_input_shape</span> <span class="o">=</span> <span class="p">{{</span><span class="s">&quot;image&quot;</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">}}};</span>
<span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span> <span class="n">opt_input_shape</span> <span class="o">=</span> <span class="p">{{</span><span class="s">&quot;image&quot;</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">}}};</span>
<span class="c1">// 设置 TensorRT 的动态 Shape</span>
<span class="n">config</span><span class="p">.</span><span class="n">SetTRTDynamicShapeInfo</span><span class="p">(</span><span class="n">min_input_shape</span><span class="p">,</span> <span class="n">max_input_shape</span><span class="p">,</span> <span class="n">opt_input_shape</span><span class="p">);</span>
</pre></div>
</div>
<p>代码示例 (3)：使用 TensorRT OSS 进行预测</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// 创建 Config 对象</span>
<span class="n">paddle_infer</span><span class="o">::</span><span class="n">Config</span> <span class="n">config</span><span class="p">(</span><span class="n">FLAGS_infer_model</span> <span class="o">+</span> <span class="s">&quot;/mobilenet&quot;</span><span class="p">);</span>

<span class="c1">// 启用 GPU 进行预测</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableUseGpu</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

<span class="c1">// 启用 TensorRT 进行预测加速</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableTensorRtEngine</span><span class="p">();</span>
<span class="c1">// 启用 TensorRT OSS 进行预测加速</span>
<span class="n">config</span><span class="p">.</span><span class="n">EnableTensorRtOSS</span><span class="p">();</span>

<span class="c1">// 通过 API 获取 TensorRT OSS 启用结果 - true</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Enable TensorRT is: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">config</span><span class="p">.</span><span class="n">tensorrt_oss_enabled</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="XPUConfig.html" class="btn btn-neutral float-right" title="5. 使用 XPU 进行预测" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="CPUConfig.html" class="btn btn-neutral float-left" title="3. 使用 CPU 进行预测" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Paddle-Inference Developer

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>